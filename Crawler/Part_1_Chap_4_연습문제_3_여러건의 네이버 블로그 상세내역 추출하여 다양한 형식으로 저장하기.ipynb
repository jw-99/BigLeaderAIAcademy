{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구 주소 블로그 : https://blog.naver.com/junhyuk_abba/221286810022\n",
    "# 신 주소 블로그 : https://blog.naver.com/suheeryu/221314766979\n",
    "\n",
    "print(\"=\" *80)\n",
    "print(\" 연습문제: 블로그  크롤러 : 네이버 view -> 블로그 정보 수집하기\")\n",
    "print(\"=\" *80)\n",
    "\n",
    "#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas  as pd    \n",
    "import math\n",
    "\n",
    "#Step 2. 필요한 정보 입력받기\n",
    "query_txt = input('1.정보를 수집할 키워드는 무엇입니까?: ')\n",
    "start_date = input('2.조회를 시작할 날짜를 입력하세요(예:2017-01-01) :')\n",
    "end_date = input('3.조회를 종료할 날짜를 입력하세요(예:2017-12-31): ')\n",
    "cnt = int(input('4.몇 건의 정보를 수집할까요? :'))\n",
    "page_cnt = math.ceil( cnt / 30 )\n",
    "f_dir = input(\"5.파일을 저장할 폴더명만 쓰세요(기본값:c:\\\\temp\\\\):\")\n",
    "if f_dir == '' :\n",
    "    f_dir=\"c:\\\\temp\\\\\"\n",
    "\n",
    "# Step 3. 결과를 저장할 파일명을 지정하기\n",
    "import os\n",
    "\n",
    "n = time.localtime()\n",
    "s = '%04d-%02d-%02d-%02d-%02d-%02d' %(n.tm_year, n.tm_mon, n.tm_mday, n.tm_hour, n.tm_min, n.tm_sec)   \n",
    "\n",
    "os.makedirs(f_dir+'NAVER_VIEW'+'-'+s+'-'+query_txt)\n",
    " \n",
    "ft_name = f_dir+'NAVER_View'+'-'+s+'-'+query_txt+'\\\\'+'NAVER_View'+'-'+s+'-'+query_txt+'.txt'\n",
    "fc_name = f_dir+'NAVER_View'+'-'+s+'-'+query_txt+'\\\\'+'NAVER_View'+'-'+s+'-'+query_txt+'.csv'\n",
    "fx_name = f_dir+'NAVER_View'+'-'+s+'-'+query_txt+'\\\\'+'NAVER_View'+'-'+s+'-'+query_txt+'.xls'    \n",
    "print(\"\\n\")  \n",
    "\n",
    "s_time = time.time( )\n",
    "\n",
    "#Step 4. 검색어 입력한 후 검색하여 View로 이동하기\n",
    "import chromedriver_autoinstaller\n",
    "chromedriver_autoinstaller.install()\n",
    "driver= webdriver.Chrome()\n",
    "\n",
    "url = 'https://www.naver.com'\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_id('query').click()\n",
    "ele = driver.find_element_by_id('query')\n",
    "ele.send_keys(query_txt)\n",
    "ele.send_keys(\"\\n\")\n",
    "time.sleep(3)\n",
    "\n",
    "driver.find_element_by_link_text('VIEW').click()\n",
    "driver.find_element_by_link_text('블로그').click()\n",
    "time.sleep(1)\n",
    "\n",
    "# 옵션 -> 날짜 입력하기\n",
    "driver.find_element_by_link_text('옵션').click()\n",
    "time.sleep(1)\n",
    "\n",
    "# 옵션 - 날짜 - 직접입력 클릭\n",
    "driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[1]/a[9]').click()\n",
    "time.sleep(0.5)\n",
    "\n",
    "# 조회 시작 날짜와 종료 날짜 선택\n",
    "start_date2 = start_date.split('-')\n",
    "start_year = start_date2[0]\n",
    "start_mon = str(int(start_date2[1]))\n",
    "start_day = str(int(start_date2[2]))\n",
    "\n",
    "end_date2 = end_date.split('-')\n",
    "end_year = end_date2[0]\n",
    "end_mon = str(int(end_date2[1]))\n",
    "end_day = str(int(end_date2[2]))\n",
    "\n",
    "#시작 날짜 부분 클릭\n",
    "driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[1]/span[1]/a').click()\n",
    "\n",
    "#시작 년도 클릭\n",
    "driver.find_element_by_link_text(start_year).click()\n",
    "time.sleep(1)\n",
    "\n",
    "#시작 월의 값은 xpath 로 선택합니다\n",
    "if start_mon == '1' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[1]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '2' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[2]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '3' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[3]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '4' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[4]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '5' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[5]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '6' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[6]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '7' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[7]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '8' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[8]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_mon == '9' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[9]/a').click()\n",
    "    time.sleep(1)\n",
    "else :\n",
    "    driver.find_element_by_link_text(start_mon).click()\n",
    "    time.sleep(1)\n",
    "\n",
    "#시작일의 값도 xpath 값을 사용합니다.\n",
    "if start_day == '1' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[1]/a').click()\n",
    "    time.sleep(1)                 \n",
    "elif start_day == '2' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[2]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '3' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[3]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '4' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[4]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '5' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[5]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '6' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[6]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '7' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[7]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '8' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[8]/a').click()\n",
    "    time.sleep(1)\n",
    "elif start_day == '9' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[9]/a').click()\n",
    "    time.sleep(1)\n",
    "else :\n",
    "    driver.find_element_by_link_text(start_day).click()\n",
    "    time.sleep(1)\n",
    "\n",
    "#종료 날짜 부분 클릭\n",
    "driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[1]/span[3]/a').click()\n",
    "\n",
    "#종료 년도 클릭\n",
    "driver.find_element_by_link_text(end_year).click()\n",
    "time.sleep(1)  \n",
    "\n",
    "#종료 월의 값은 xpath 로 선택합니다\n",
    "if end_mon == '1' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[1]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '2' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[2]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '3' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[3]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '4' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[4]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '5' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[5]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '6' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[6]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '7' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[7]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '8' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[8]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_mon == '9' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[2]/div/div/div/ul/li[9]/a').click()\n",
    "    time.sleep(1)\n",
    "else :\n",
    "    driver.find_element_by_link_text(end_mon).click()\n",
    "    time.sleep(1)\n",
    "\n",
    "#종료일의 값도 xpath 값을 사용합니다.\n",
    "if end_day == '1' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[1]/a').click()\n",
    "    time.sleep(1)                 \n",
    "elif end_day == '2' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[2]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '3' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[3]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '4' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[4]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '5' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[5]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '6' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[6]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '7' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[7]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '8' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[8]/a').click()\n",
    "    time.sleep(1)\n",
    "elif end_day == '9' :\n",
    "    driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[2]/div[3]/div/div/div/ul/li[9]/a').click()\n",
    "    time.sleep(1)\n",
    "else :\n",
    "    driver.find_element_by_link_text(end_day).click()\n",
    "    time.sleep(1)\n",
    "\n",
    "# 날짜 입력 후 적용 버튼 클릭        \n",
    "driver.find_element_by_xpath('//*[@id=\"snb\"]/div[2]/ul/li[3]/div/div[2]/div[3]/button').click()\n",
    "time.sleep(1)\n",
    "\n",
    "# Step 5. 검색 요청 건수만큼 화면 스크롤링 하기\n",
    "# 자동 스크롤다운 함수\n",
    "def scroll_down(driver):\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "\n",
    "i = 1\n",
    "while (i <= page_cnt+2):\n",
    "    scroll_down(driver) \n",
    "    i += 1\n",
    "    print('%s 페이지 정보를 추출하고 있으니 잠시만 기다려 주세요~~^^' %i)\n",
    "\n",
    "# Step 6. 현재 조회된 목록에서 URL 주소를 추출하여 리스트 생성하기\n",
    "url_all_list=[]    #조회할 블로그의 URL 정보 저장용 리스트\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "url_list_1 = soup.find('ul','lst_total').find_all('li')\n",
    "\n",
    "for a in url_list_1 :\n",
    "    url_all_list.append( a.find('div','total_area').find_all('a') )\n",
    "\n",
    "url_detail=[]\n",
    "for b in range(0,len(url_all_list)) :\n",
    "    url_detail.append( url_all_list[b][5]['href'] )\n",
    "\n",
    "url_final_list=[]   # 수집할 블로그 리스트를 저장할 변수\n",
    "no = 1\n",
    "for c in url_detail :    \n",
    "    if c.split('/')[2] == 'blog.naver.com' :\n",
    "        url_final_list.append(c)\n",
    "        no += 1\n",
    "\n",
    "        if no > cnt :\n",
    "            break\n",
    "\n",
    "d_no = 1\n",
    "print('')\n",
    "print('정보를 수집할 블로그 URL 주소는 아래와 같습니다~~~')\n",
    "print('')\n",
    "for d in url_final_list :   \n",
    "    print(d_no,':',d)\n",
    "    d_no += 1\n",
    "\n",
    "driver.close()\n",
    "print('')\n",
    "\n",
    "#Step 7. 수집된 URL 주소에 접속하여 데이터 추출하기\n",
    "import chromedriver_autoinstaller\n",
    "chromedriver_autoinstaller.install()\n",
    "driver= webdriver.Chrome()\n",
    "\n",
    "blog_addr2 = []\n",
    "w_name2 = []\n",
    "w_date2 = []\n",
    "blog_txt2 = []\n",
    "\n",
    "# driver.delete_all_cookies()\n",
    "# time.sleep(2)\n",
    "\n",
    "no = 1   # 전체 게시글 번호용 변수\n",
    "\n",
    "for blog_addr in url_final_list :            \n",
    "    driver.get(blog_addr)\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.switch_to.frame('mainFrame')\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    addr_1 = soup.select('#postViewArea')\n",
    "    addr_2 = soup.select('div[class=\"se_component_wrap sect_dsc __se_component_area\"]')\n",
    "    addr_3 = soup.select('div[class=\"se-main-container\"]')\n",
    "\n",
    "    if addr_1 :\n",
    "        print(\"%s번째 게시글 정보를 수집합니다~~~~~~~~~~\" %no)\n",
    "        f = open(ft_name, 'a',encoding='UTF-8')\n",
    "        \n",
    "        # 블로그 URL 주소\n",
    "        print(\"1.블로그주소: \",blog_addr)\n",
    "        blog_addr2.append(blog_addr)\n",
    "        f.write(\"1.블로그 주소:\"+ blog_addr + \"\\n\")\n",
    "        \n",
    "        # 작성자 닉네임\n",
    "        writer = soup.select(\"div.blog2_container > span.writer\")\n",
    "        try :\n",
    "                wname = writer[0].get_text( )   # 작성자 닉네임\n",
    "        except IndexError :\n",
    "                wname = \"작성자 닉네임이 없습니다\"\n",
    "        else :\n",
    "                wname = wname.replace(\"\\n\",\"\")\n",
    "\n",
    "        print(\"2.작성자 닉네임: \",wname )\n",
    "        w_name2.append(wname)\n",
    "        f.write(\"2.작성자 닉네임:\" + wname + \"\\n\")\n",
    "        \n",
    "        # 작성일자\n",
    "        wdate = soup.select('p[class=\"date fil5 pcol2 _postAddDate\"]')\n",
    "        try : \n",
    "                wdate = wdate[0].get_text( )\n",
    "        except IndexError :\n",
    "                wdate = ' '\n",
    "\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        w_date2.append(wdate)\n",
    "        f.write(\"3.작성 일자:\" + wdate + \"\\n\")\n",
    "\n",
    "        # 블로그 본문 내용\n",
    "        for i in addr_1:\n",
    "                blog_txt = i.text.replace(\"\\n\",\"\")\n",
    "                print(\"4.블로그내용: \\n\",blog_txt) # 엔터키를 제거하는 코드\n",
    "                print(\"\\n\")\n",
    "                blog_txt2.append(blog_txt)\n",
    "                f.write(\"4.블로그 내용:\" + blog_txt+\"\\n\"+\"\\n\")\n",
    "\n",
    "        f.close()\n",
    "        no += 1\n",
    "\n",
    "    elif addr_2 :\n",
    "        print(\"%s번째 게시글 정보를 수집합니다~~~~~~~~~~\" %no)\n",
    "        f = open(ft_name, 'a',encoding='UTF-8')\n",
    "        \n",
    "        # 블로그 주소\n",
    "        print(\"1.블로그주소: \",blog_addr)\n",
    "        blog_addr2.append(blog_addr)\n",
    "        f.write(\"1.블로그 주소:\"+ blog_addr + \"\\n\")\n",
    "        \n",
    "        # 작성자 닉네임\n",
    "        writer = soup.select(\"div.blog2_container > span.writer\")\n",
    "        try :\n",
    "                wname = writer[0].get_text( )   # 작성자 닉네임\n",
    "        except IndexError :\n",
    "                wname = \"작성자 닉네임이 없습니다\"\n",
    "        else :\n",
    "                wname = wname.replace(\"\\n\",\"\")\n",
    "\n",
    "        print(\"2.작성자 닉네임: \",wname )\n",
    "        w_name2.append(wname)\n",
    "        f.write(\"2.작성자 닉네임:\" + wname + \"\\n\")\n",
    "        \n",
    "        # 작성일자\n",
    "        wdate = soup.select(\"div.blog2_container > span.se_publishDate.pcol2\")\n",
    "        try : \n",
    "                wdate = wdate[0].get_text( )\n",
    "        except IndexError :\n",
    "                wdate = ' '\n",
    "\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        w_date2.append(wdate)\n",
    "        f.write(\"3.작성 일자:\" + wdate + \"\\n\")\n",
    "\n",
    "        # 블로그 본문 내용\n",
    "        for i in addr_2:\n",
    "                blog_txt = i.text.replace(\"\\n\",\"\")\n",
    "                print(\"4.블로그내용: \\n\",blog_txt) \n",
    "                print(\"\\n\")\n",
    "                blog_txt2.append(blog_txt)\n",
    "                f.write(\"4.블로그 내용:\" + blog_txt+\"\\n\"+\"\\n\")\n",
    "\n",
    "        f.close( )\n",
    "        no += 1\n",
    "            \n",
    "    elif addr_3 :\n",
    "        print(\"%s번째 게시글 정보를 수집합니다~~~~~~~~~~\" %no)\n",
    "        f = open(ft_name, 'a',encoding='UTF-8')\n",
    "        \n",
    "        # 블로그 주소\n",
    "        print(\"1.블로그주소: \",blog_addr)\n",
    "        blog_addr2.append(blog_addr)\n",
    "        f.write(\"1.블로그 주소:\"+ blog_addr + \"\\n\")\n",
    "        \n",
    "        # 작성자 닉네임\n",
    "        writer = soup.select(\"div.blog2_container > span.writer\")\n",
    "        try :\n",
    "                wname = writer[0].get_text( )   # 작성자 닉네임\n",
    "        except IndexError :\n",
    "                wname = \"작성자 닉네임이 없습니다\"\n",
    "        else :\n",
    "                wname = wname.replace(\"\\n\",\"\")\n",
    "\n",
    "        print(\"2.작성자 닉네임: \",wname )\n",
    "        w_name2.append(wname)\n",
    "        f.write(\"2.작성자 닉네임:\" + wname + \"\\n\")\n",
    "        \n",
    "        # 작성일자\n",
    "        wdate = soup.select(\"div.blog2_container > span.se_publishDate.pcol2\")\n",
    "        try : \n",
    "                wdate = wdate[0].get_text( )\n",
    "        except IndexError :\n",
    "                wdate = ' '\n",
    "\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        w_date2.append(wdate)\n",
    "        f.write(\"3.작성 일자:\" + wdate + \"\\n\")\n",
    "        \n",
    "        #블로그 본문 내용\n",
    "        for i in addr_3:\n",
    "                blog_txt = i.text.replace(\"\\n\",\"\")\n",
    "                print(\"4.블로그내용: \\n\",blog_txt) \n",
    "                print(\"\\n\")\n",
    "                blog_txt2.append(blog_txt)\n",
    "                f.write(\"4.블로그 내용:\" + blog_txt+\"\\n\"+\"\\n\")\n",
    "\n",
    "        f.close( )\n",
    "        no += 1\n",
    "                   \n",
    "#Step 8. xls 형태와 csv 형태로 저장하기\n",
    "naver_blog = pd.DataFrame()\n",
    "naver_blog['블로그주소']=blog_addr2\n",
    "naver_blog['작성자닉네임']=w_name2\n",
    "naver_blog['작성일자']=w_date2\n",
    "naver_blog['블로그내용']=blog_txt2\n",
    "\n",
    "# csv 형태로 저장하기\n",
    "naver_blog.to_csv(fc_name,encoding=\"utf-8-sig\",index=False)\n",
    "\n",
    "# 엑셀 형태로 저장하기\n",
    "naver_blog.to_excel(fx_name ,index=False)\n",
    "\n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "\n",
    "print(\"\\n\") \n",
    "print(\"=\" *80)\n",
    "print(\"총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"파일 저장 완료: txt 파일명 : %s \" %ft_name)\n",
    "print(\"파일 저장 완료: csv 파일명 : %s \" %fc_name)\n",
    "print(\"파일 저장 완료: xls 파일명 : %s \" %fx_name)\n",
    "print(\"=\" *80)\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
